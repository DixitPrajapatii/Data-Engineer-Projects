Olympic data analytics using microsoft azure.

1. Download the csv datafiles from the kaggle with the following link:
    - https://www.kaggle.com/datasets/arjunprasadsarkhel/2021-olympics-in-tokyo
    - There are five files names as athletes, coaches,entriesGender, Medals and Teams.

2. Create microsoft azure account.

3. Then we need to create azure storage account to store our data.
4. GO and search in azure plateform about storage account and create the storage account.
5. Now go to that storage account and check out for container. click on container and create container.
6. GO to that container and create two folder name as raw-data for storing our raw data and transformed-data for storing transformed data.
7. Then now we need to create azure data factory so seach for the azure data factory and create azure data factory. Provide necessary information and create it.
8. Once you create data factory it will give you that launch data factory. click on that and it will redirect you to data factory page where we can build our data pipeline.
9. Now we need to create pipeline so follow the below steps.
10. Click on author and then + icon there .
11. Now click on pipeline.
12. rename the name of the pipeline as we are using this for ingestion.
13. Now go to github and create the acccount if you dont have and create repository
14. Now upload our data to github.
15. go to our data and click on Raw then copy the url which you get after clicking raw.
16. Now go to azure and click on move and transform in activity tab inside data factory.
17.drag and drop copy data and configure the necessary steps.
18. go to option in copy data and add source data and search for http
19. select it and add csv file as type.
20. now add the name and now we need to add link of the source file.
21. copy and  paste our url into base url section under add link (newlink)
22.inside the authentication add annonymous.
23. create the link this will create the link.
24. Now update the name and click on sink
25. now we need to add sink dataset so click on new and search for azure gen 2 storage account and select it.
26. then select csv in file type.
27. then we can create linked service which is link to azure storage gen 2 account.
28. click on new link service
29. add name , subscription, storage account name details in that and click on create.
30. After creating link service you will be able to see the details. so click on that link and select file path.
31. click on file icon shown near empty brackets and browse the path and select it.
32. provide the file name with .csv extenstion
33.click ok 
34. now validate. once validate successfully then click on debug. once the debug completed then you will be able to see the file inside storage account.
35. now repeate this step for each file 
36. Once created all the copy resource click on validate and then debug. Once successfully completed then you will be able to see all the files inside gen 2 storage account row folder.

# Databricks
1. Go to azure and search for azure databricks and open it.
2. click on create azure databricks services.
3. provide all the necessary information and create databricks.
4. once cretes go to databricks and click on launch workspace.
5. Now we need to create compute so click on compute and click on create.
6. provide all the information and click on create.
7. Once completed successfully you can use that cluster.
8. Now we need to do transformation our data which is store in azure gen 2 storage account.
9. Now click on new and click on new notebook.
10. rename the notebook name and add code mentioned in transformation.ipynb file.
11. now we need to configure our storage account with databricks so follow the below code steps to do that.
    - GO to azure home page and search for app registration. 
    - click on new registration
    - provide name and click on registor. it will registor the app.
    - once you created you will get client id and tenant id copy in notebook for reference.
    - click on certificate and secrets.
    - now click on new client secret. and provide name and create.
    - now copy the secret key inside out text document where we store our client id.
    - now update credential in ipynb file config ids and try to run the whole code one after one.

dc29f1db-0fc8-42bc-a286-590e80375d91 c
ab458f1f-d2f6-4ad8-a79e-40fc9cf54d71
Note: azure provide key vault to store your important credential like password, keys and all.
12. now update the below code 
    source = "abfss://olympic-data@olympicdatastoragedx.dfs.core.windows.net",
    add container name and storage account name (olympic-data@olympicdatastoragedx)
13. once you run above successfully you will get error in the following code
        %fs
        ls "/mnt/tokyoolymic"
    - permission issue.
    => follow the steps to resolve the issue.
        - go to container click on access control on left side iam .
        - add role assignment.add the storage blob data contributor.
        - click on add role assignement and search for above data contributor role and next.
        - now select member which is our app and add it.
        - now review and create.
        - once you create go to databricks and run the list code again to check the file inside databricks. it will show the details of files.
14. now we need to read file 
- spark and run it
- you can follow the end code which is provided in ipynb file till end.


Part - 2 
1. Go to azure and search for azure synapse analytics
2. create azure synapse analytics and provide all the information. (In the account information add our storage account which we created or else 
    create new azure gen 2 storage account where azure can store metadata file which the operation we are going to perform in synapse analytics.)
3. Once you successfully created the synapse analytics, go to resouce and click on synapse analytics where you can see all the information like 
    activity log, access control, tag , setting , analytics pool, security and etc.
4. In the overview section you will be able to see azure synapse studio , click on that and it will redirect you to azure synapse studio.
5. Now we need to create database so click on data and in + icon click and go for lake database. It will create database and update the name as per the requirement.
6. Now we need to add our all csv file and create table inside the synapse so follow the below steps.
    1. now click on table + icon and add from data lake.
    2. provide external table name as the csv file we have and provide the linked service.
    3. once you add linked service it will ask for file or folder to select click on that and redirect to our transformed data and click on csv file and select it.
    4. Once you select the file it will show the structure / schema of the csv.
    5. make sure to click on first row infer column names once you select the csv it will show this option in new external table.
    6.click on create and it will create table with the desire data we have inside csv.
    7. Now repeate this steps for all our file and make sure column name does not containt any space 
7. Once you completed above successfully we are able to fetch each and every table data using sql queries. Make sure all the table data are correct.
8. Now we need to do analytics on top of that so follow the file name as SQL Data Analytics Task.
9. Now we can connect our data storage with power bi and create a visual on top of that.
10. We have successfully completed our project for olympic data analytics on microsoft azure using azure services as well as azure synapse analytics.
